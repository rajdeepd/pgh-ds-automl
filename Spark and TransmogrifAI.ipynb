{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "[Spark](#spark)\n",
    "\n",
    "[TransmogrifAI](#automl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scala\"></a>\n",
    "## Scala\n",
    "\n",
    "[Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "version 2.11.12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scala.util.Properties.versionString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"automl\"></a>\n",
    "## Get TransmogrifAI\n",
    "\n",
    "[Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a458e0-e991-4534-ba61-f57ace682403",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add mvn com.salesforce.transmogrifai transmogrifai-core_2.11 0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"spark\"></a>\n",
    "## Get Spark \n",
    "\n",
    "[Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfa90e4-f3f1-4376-a9fb-e2e3b3d18bde",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add mvn org.apache.spark spark-mllib_2.11 2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing mllib gets spark-core, spark-mllib and spark-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.SparkContext\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import com.salesforce.op._\n",
       "import com.salesforce.op.features._\n",
       "import com.salesforce.op.features.types._\n",
       "import com.salesforce.op.stages.impl.classification._\n",
       "import com.salesforce.op.evaluators.Evaluators\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.salesforce.op._\n",
    "import com.salesforce.op.features._\n",
    "import com.salesforce.op.features.types._\n",
    "import com.salesforce.op.stages.impl.classification._\n",
    "import com.salesforce.op.evaluators.Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.log4j.Logger.getRootLogger().setLevel(org.apache.log4j.Level.WARN);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession$implicits$@621dbf6d"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"automl-app\") // Spark configuration\n",
    "val sc = new SparkContext(conf)  // initialize spark context\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)  // initialize sql context\n",
    "implicit val spark = SparkSession.builder.config(conf).getOrCreate() // start spark session \n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "Using the SparkSQL Context, we'll read in the titanic dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawData = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawData.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
      "[1,0,3,Braund, Mr. Owen Harris,male,22.0,1,0,A/5 21171,7.25,null,S]\n",
      "[2,1,1,Cumings, Mrs. John Bradley (Florence Briggs Thayer),female,38.0,1,0,PC 17599,71.2833,C85,C]\n",
      "[3,1,3,Heikkinen, Miss. Laina,female,26.0,0,0,STON/O2. 3101282,7.925,null,S]\n",
      "[4,1,1,Futrelle, Mrs. Jacques Heath (Lily May Peel),female,35.0,1,0,113803,53.1,C123,S]\n",
      "[5,0,3,Allen, Mr. William Henry,male,35.0,0,0,373450,8.05,null,S]\n"
     ]
    }
   ],
   "source": [
    "println(rawData.columns.mkString(\",\"))\n",
    "rawData.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note - we want to get a model in a few steps as possible, so we will require that any numeric variable will be a double.  This is important for when we read the data into our feature engineering.  It will, by default, handle one type of numeric type or a string.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: double, survived: double ... 10 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cast all non doulbe numeric types to double\n",
    "\n",
    "\n",
    "rawData.createOrReplaceTempView(\"raw\")\n",
    "val passengerData = spark.sql(\"\"\"\n",
    "    select \n",
    "      cast(passengerId as double) as id, \n",
    "      cast(survived as double) as survived, \n",
    "      cast(pclass as double) as pclass,\n",
    "      name, sex, age, \n",
    "      cast(sibsp as double) as sibsp, \n",
    "      cast(parch as double) as parch, \n",
    "      ticket, fare, cabin, embarked \n",
    "      from raw\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare target and features. \n",
    "\n",
    "Below, The `FeatureBuilder` object has a method `fromDataFrame`, to which we pass the DataFrame `passengerData` created above as well as the name of the target variable, in this case `survived`.  It is important to point out that the `RealNN` is a type parameterization (well beyond the scope of this talk), but in short, it will make it a requirement that all fields in the DataFrame which are numeric to have a Double data type.  If not you will get an error.  It should be clear that every field which is not `survived` will be treated as a potential feature in the feature engineering section.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (target, features) = FeatureBuilder.fromDataFrame[RealNN](passengerData, response = \"survived\")\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `features` variables which is returned in the previous cell is a sequence of features.  And as is, it is not entirely useful.  Our ML algorithms was numbers, and we still have text floating around in our feature set.  So we will need to perform one hot encodings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature(name = id, uid = Real_000000000001, isResponse = false, originStage = FeatureGeneratorStage_000000000001, parents = [], distributions = [])\n",
      "\n",
      "Feature(name = pclass, uid = Real_000000000003, isResponse = false, originStage = FeatureGeneratorStage_000000000003, parents = [], distributions = [])\n",
      "\n",
      "Feature(name = name, uid = Text_000000000004, isResponse = false, originStage = FeatureGeneratorStage_000000000004, parents = [], distributions = [])\n",
      "\n",
      "Feature(name = sex, uid = Text_000000000005, isResponse = false, originStage = FeatureGeneratorStage_000000000005, parents = [], distributions = [])\n",
      "\n",
      "Feature(name = age, uid = Real_000000000006, isResponse = false, originStage = FeatureGeneratorStage_000000000006, parents = [], distributions = [])\n",
      "\n",
      "Feature(name = sibsp, uid = Real_000000000007, isResponse = false, originStage = FeatureGeneratorStage_000000000007, parents = [], distributions = [])\n",
      "\n",
      "Feature(name = parch, uid = Real_000000000008, isResponse = false, originStage = FeatureGeneratorStage_000000000008, parents = [], distributions = [])\n",
      "\n",
      "Feature(name = ticket, uid = Text_000000000009, isResponse = false, originStage = FeatureGeneratorStage_000000000009, parents = [], distributions = [])\n",
      "\n",
      "Feature(name = fare, uid = Real_00000000000a, isResponse = false, originStage = FeatureGeneratorStage_00000000000a, parents = [], distributions = [])\n",
      "\n",
      "Feature(name = cabin, uid = Text_00000000000b, isResponse = false, originStage = FeatureGeneratorStage_00000000000b, parents = [], distributions = [])\n",
      "\n",
      "Feature(name = embarked, uid = Text_00000000000c, isResponse = false, originStage = FeatureGeneratorStage_00000000000c, parents = [], distributions = [])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.foreach(i => println(i + \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transmogrify\n",
    "\n",
    "Our next step with be to transmogrify the sequence of features to a feature vector.  \n",
    "\n",
    "This is like sparks VectorAssembler on steriods. \n",
    "\n",
    "The `transmogrify` method takes in a sequence of features, automatically applies default transformations to them based on feature types (e.g. imputation, null value tracking, one hot encoding, tokenization, split Emails and pivot out the top K domains) and combines them into a single vector.\n",
    "\n",
    "Mind you, we have not declared any special types other than text, so it will probably just do tokeniziation and to bag of words, null value tracking and maybe som imputation on the numeric data types will null values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature(name = age-cabin-embarked-fare-id-name-parch-pclass-sex-sibsp-ticket_3-stagesApplied_OPVector_00000000000f, uid = OPVector_00000000000f, isResponse = false, originStage = VectorsCombiner_00000000000f, parents = [OPVector_00000000000d,OPVector_00000000000e], distributions = [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureVector = features.transmogrify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the `featureVector.toString`, we can see the name of our feature vector.  It is essentially a concatenation of all the fields that we said were features as well as the total number of stages that have been appied.  It provides no insight into the transmogrified feature vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature(name = age-fare-id-parch-pclass-sibsp_1-stagesApplied_OPVector_00000000000d, uid = OPVector_00000000000d, isResponse = false, originStage = RealVectorizer_00000000000d, parents = [Real_000000000001,Real_000000000003,Real_000000000006,Real_000000000007,Real_000000000008,Real_00000000000a], distributions = [])\n",
      "\n",
      "Feature(name = cabin-embarked-name-sex-ticket_1-stagesApplied_OPVector_00000000000e, uid = OPVector_00000000000e, isResponse = false, originStage = SmartTextVectorizer_00000000000e, parents = [Text_000000000004,Text_000000000005,Text_000000000009,Text_00000000000b,Text_00000000000c], distributions = [])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featureVector.parents.foreach(v => println(v + \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transmogrify for numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just a few examples of options for transmogrify on numeric variables\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fillValue: default value for FillWithConstant (default: 0.0)\n",
       "\n",
       "inputFeatures: Input features (default: [Lcom.salesforce.op.features.TransientFeature;@c8a7849, current: [Lcom.salesforce.op.features.TransientFeature;@7e733549)\n",
       "\n",
       "inputSchema: the schema of the input data from the dataframe (default: StructType())\n",
       "\n",
       "outputFeatureName: output name that overrides default output name for feature made by this stage (undefined)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"just a few examples of options for transmogrify on numeric variables\")\n",
    "featureVector.parents(0).originStage.explainParams.split(\"\\n\").take(4).mkString(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transmogrify for non numeric variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just a few examples of options for transmogrify on non numeric variables\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "autoDetectLanguage: whether to attempt language detection (default: false, current: false)\n",
       "\n",
       "autoDetectThreshold: language detection threshold (default: 0.99, current: 0.99)\n",
       "\n",
       "binaryFreq: if true, term frequency vector will be binary such that non-zero term counts will be set to 1.0 (default: false, current: false)\n",
       "\n",
       "cleanText: ignore capitalization and punctuation in grouping categories (default: true, current: true)\n",
       "\n",
       "defaultLanguage: default language (default: unknown, current: unknown)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"just a few examples of options for transmogrify on non numeric variables\")\n",
    "featureVector.parents(1).originStage.explainParams.split(\"\\n\").take(5).mkString(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checker\n",
    "\n",
    "The Sanity Checker is a TransmogrifAI estimator that will help identify problems in your dataset.  The easiest way to use it is to call the `sanityCheck` method of the target variable.  You need to provide, as the first argument, the feature vector.  There are many other arguments you can pass in, but on removing bad features as determined by the checks performed.  \n",
    "\n",
    "Some other options \n",
    "* Max correlation allowed between a feature and the target (default is 0.95)\n",
    "* Min correlation allowed between a feature and the target (default is 0.0)\n",
    "* Minimum variance allowed for a feature vector (default is 1e-5)\n",
    "* Set the type of correlation to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature(name = age-cabin-embarked-fare-id-name-parch-pclass-sex-sibsp-survived-ticket_4-stagesApplied_OPVector_000000000010, uid = OPVector_000000000010, isResponse = false, originStage = SanityChecker_000000000010, parents = [RealNN_000000000002,OPVector_00000000000f], distributions = [])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val checkedFeatures = target.sanityCheck(featureVector, removeBadFeatures = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just an example of a few things to to set in the sanity checker\n",
      "\n",
      "categoricalLabel: If true, then label is treated as categorical (eg. Cramer's V will be calculated between it and categorical features). If this is not set, then use a max class fraction of 0.1 to estimate whether label iscategorical or not. (undefined)\n",
      "\n",
      "checkSample: Rate to downsample the data for statistical calculations (note: actual sampling will not be exact due to Spark's dataset sampling behavior) (default: 1.0, current: 1.0)\n",
      "\n",
      "correlationExclusion: Setting for what categories of feature vector columns to exclude from the correlation calculation (default: NoExclusion, current: NoExclusion)\n",
      "\n",
      "correlationType: Which coefficient to use for computing correlation (default: Pearson, current: Pearson)\n",
      "\n",
      "featureLabelCorrOnly: If true, then only calculate the correlations between the features and the label. Otherwise, calculate the entire correlation matrix, which includes all feature-feature correlations. (default: false, current: false)\n"
     ]
    }
   ],
   "source": [
    "println(\"just an example of a few things to to set in the sanity checker\\n\")\n",
    "println(checkedFeatures.originStage.explainParams.split(\"\\n\").take(5).mkString(\"\\n\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature(name = prediction, uid = Prediction_00000000001c, isResponse = true, originStage = ModelSelector_00000000001c, parents = [RealNN_000000000002,OPVector_000000000010], distributions = [])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prediction = BinaryClassificationModelSelector.\n",
    "withCrossValidation(seed=142L)\n",
    ".setInput(target, checkedFeatures).setOutputFeatureName(\"prediction\").getOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: double, survived: double ... 10 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = passengerData.randomSplit( Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.salesforce.op.OpWorkflow@770bea6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Setting up a TransmogrifAI workflow\n",
    "val workflow = new OpWorkflow().setInputDataset(train).setResultFeatures(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.salesforce.op.OpWorkflowModel@226780f0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fittedWorkflow = workflow.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "Evaluated OpLinearSVC, OpLogisticRegression, OpGBTClassifier, OpRandomForestClassifier models using Cross Validation and area under precision-recall metric.\n",
      "Evaluated 4 OpLinearSVC models with area under precision-recall metric between [0.7552145705442013, 0.7671350429397218].\n",
      "Evaluated 8 OpLogisticRegression models with area under precision-recall metric between [0.7520247737394954, 0.824238632921519].\n",
      "Evaluated 18 OpGBTClassifier models with area under precision-recall metric between [0.7831357620264897, 0.8353950079854915].\n",
      "Evaluated 18 OpRandomForestClassifier models with area under precision-recall metric between [0.699034090450801, 0.8278481003697127].\n",
      "+--------------------------------------------------------+\n",
      "|            Selected Model - OpGBTClassifier            |\n",
      "+--------------------------------------------------------+\n",
      "| Model Param           | Value                          |\n",
      "+-----------------------+--------------------------------+\n",
      "| cacheNodeIds          | false                          |\n",
      "| checkpointInterval    | 10                             |\n",
      "| featureSubsetStrategy | all                            |\n",
      "| impurity              | gini                           |\n",
      "| lossType              | logistic                       |\n",
      "| maxBins               | 32                             |\n",
      "| maxDepth              | 3                              |\n",
      "| maxIter               | 20                             |\n",
      "| maxMemoryInMB         | 256                            |\n",
      "| minInfoGain           | 0.001                          |\n",
      "| minInstancesPerNode   | 10                             |\n",
      "| modelType             | OpGBTClassifier                |\n",
      "| name                  | OpGBTClassifier_000000000017_0 |\n",
      "| seed                  | 1918769265                     |\n",
      "| stepSize              | 0.1                            |\n",
      "| subsamplingRate       | 1.0                            |\n",
      "| uid                   | OpGBTClassifier_000000000017   |\n",
      "+-----------------------+--------------------------------+\n",
      "+-------------------------------------------------------------------------+\n",
      "|                        Model Evaluation Metrics                         |\n",
      "+-------------------------------------------------------------------------+\n",
      "| Metric Name                 | Training Set Value  | Hold Out Set Value  |\n",
      "+-----------------------------+---------------------+---------------------+\n",
      "| area under ROC              | 0.9335071707953064  | 0.8795948987246813  |\n",
      "| area under precision-recall | 0.9176366757147773  | 0.8799618715478554  |\n",
      "| brier score                 | 0.0974938657364445  | 0.1317018048145673  |\n",
      "| error                       | 0.12521739130434784 | 0.16216216216216217 |\n",
      "| f1                          | 0.8325581395348837  | 0.7999999999999999  |\n",
      "| false negative              | 42.0                | 7.0                 |\n",
      "| false positive              | 30.0                | 5.0                 |\n",
      "| precision                   | 0.8564593301435407  | 0.8275862068965517  |\n",
      "| recall                      | 0.8099547511312217  | 0.7741935483870968  |\n",
      "| true negative               | 324.0               | 38.0                |\n",
      "| true positive               | 179.0               | 24.0                |\n",
      "+-----------------------------+---------------------+---------------------+\n",
      "+---------------------------------------------------+\n",
      "|                Top Model Insights                 |\n",
      "+---------------------------------------------------+\n",
      "| Top Positive Correlations |     Correlation Value |\n",
      "+---------------------------+-----------------------+\n",
      "| sex(sex = Female)         |    0.5584557671113016 |\n",
      "| name                      |    0.3564692834912751 |\n",
      "| fare                      |     0.249371075552379 |\n",
      "| embarked(embarked = C)    |   0.20790363000945558 |\n",
      "| ticket                    |   0.20553762847702012 |\n",
      "| parch                     |   0.09568627605410927 |\n",
      "| cabin                     |   0.09165752102954867 |\n",
      "| embarked(embarked = null) |   0.05282622149337183 |\n",
      "| embarked(embarked = Q)    | -0.009250734572346883 |\n",
      "| id                        | -0.015451386584771756 |\n",
      "| sibsp                     |  -0.03146485641077983 |\n",
      "| age                       |  -0.05128944339547986 |\n",
      "| age(age = null)           |  -0.09301641944784111 |\n",
      "| embarked(embarked = S)    |  -0.18086082894590652 |\n",
      "| cabin(cabin = null)       |  -0.31591211290300675 |\n",
      "+---------------------------+-----------------------+\n",
      "+---------------------------------------------------+\n",
      "| Top Negative Correlations |     Correlation Value |\n",
      "+---------------------------+-----------------------+\n",
      "| sex(sex = Male)           |    -0.558455767111302 |\n",
      "| name                      |   -0.5529888904935159 |\n",
      "| pclass                    |  -0.35013762569592227 |\n",
      "| ticket                    |  -0.11034465379577141 |\n",
      "| cabin                     | -0.046680144599850454 |\n",
      "+---------------------------+-----------------------+\n",
      "+--------------------------------------------------+\n",
      "| Top Contributions         |   Contribution Value |\n",
      "+---------------------------+----------------------+\n",
      "| fare                      |   0.1742720350265502 |\n",
      "| age                       |  0.11629932679154206 |\n",
      "| name                      |  0.11286321280534772 |\n",
      "| sibsp                     |  0.09854789391949734 |\n",
      "| id                        |   0.0711214488513699 |\n",
      "| embarked(embarked = S)    |  0.06207427634698938 |\n",
      "| parch                     | 0.058762399167571255 |\n",
      "| cabin(cabin = null)       |  0.05435520776922696 |\n",
      "| ticket                    | 0.047077482962876546 |\n",
      "| sex(sex = Male)           | 0.018967792934566123 |\n",
      "| embarked(embarked = C)    |  0.01679960092773063 |\n",
      "| pclass                    | 0.013094093502807458 |\n",
      "| age(age = null)           |  0.01015905997740652 |\n",
      "| embarked(embarked = Q)    | 7.669690193820319E-4 |\n",
      "| embarked(embarked = null) |                  0.0 |\n",
      "+---------------------------+----------------------+\n",
      "+------------------------------------+\n",
      "| Top CramersV |            CramersV |\n",
      "+--------------+---------------------+\n",
      "| sex          |  0.5584557671113016 |\n",
      "| cabin        |  0.3159121129030065 |\n",
      "| embarked     | 0.21709833473950063 |\n",
      "| age          | 0.09301641944784106 |\n",
      "+--------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "println(\"Model summary:\\n\" + fittedWorkflow.summaryPretty())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpBinaryClassificationEvaluator_000000000043"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = Evaluators.BinaryClassification()\n",
    "   .setLabelCol(target)\n",
    "   .setPredictionCol(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.salesforce.op.OpWorkflowModel@226780f0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fittedWorkflow.setInputDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (scoredTestData, metrics) = fittedWorkflow.scoreAndEvaluate(evaluator = evaluator)\n",
    "OutputCell.HIDDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7662e9-668f-49a3-902c-b078927c6463",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics.toMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: double, survived: double ... 10 more fields]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cast all non doulbe numeric types to double\n",
    "\n",
    "val rawTestData = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"test.csv\")\n",
    "rawTestData.createOrReplaceTempView(\"rawTest\")\n",
    "val passengerTestData = spark.sql(\"\"\"\n",
    "    select \n",
    "      cast(passengerId as double) as id, \n",
    "      cast(1 as double) as survived, \n",
    "      cast(pclass as double) as pclass,\n",
    "      name, sex, age, \n",
    "      cast(sibsp as double) as sibsp, \n",
    "      cast(parch as double) as parch, \n",
    "      ticket, fare, cabin, embarked \n",
    "      from rawTest\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: double, prediction: map<string,double>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fittedWorkflow.setInputDataset(passengerTestData)\n",
    "\n",
    "val output = fittedWorkflow.computeDataUpTo(prediction).select(\"id\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.{File, PrintWriter}\n",
    "\n",
    "val local = output.rdd.map{\n",
    "    row => (row.get(0), row.getAs[Map[String,Double]](1))\n",
    "}.collect.map{case(x,y) => (x, y.get(\"probability_1\").get)}.map{ \n",
    " case(x,y) => (x.asInstanceOf[Double].toInt, if(y > 0.5) 1 else 0)\n",
    "}\n",
    "\n",
    "val myFile = new File(\"myprediction.csv\")\n",
    "val pw = new PrintWriter(myFile)\n",
    "pw.write(\"PassengerId,Survived\\n\")\n",
    "for(i <- local) { \n",
    "    val (t1, t2) = i\n",
    "    pw.write(s\"$t1,$t2\\n\")\n",
    "}\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This entry got me rankied 1806 out of 10379\n",
    "\n",
    "![](kaggle-placement.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I honestly don't remember scoring 0.78468, but I'm certain I put a load of time into it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Task not serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31morg.apache.spark.SparkException: Task not serializable\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:345)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.SparkContext.clean(SparkContext.scala:2292)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:844)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:843)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:843)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:608)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.show(Dataset.scala:691)\u001b[0;0m",
      "\u001b[1;31m  ... 54 elided\u001b[0;0m",
      "\u001b[1;31mCaused by: java.io.NotSerializableException: com.salesforce.op.OpWorkflowModel\u001b[0;0m",
      "\u001b[1;31mSerialization stack:\u001b[0;0m",
      "\u001b[1;31m\t- object not serializable (class: com.salesforce.op.OpWorkflowModel, value: com.salesforce.op.OpWorkflowModel@226780f0)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: fittedWorkflow, type: class com.salesforce.op.OpWorkflowModel)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@709ce841)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@7376e8ad)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@3ded5187)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@65f2e6fa)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@56201492)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@3d7d1155)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@1f612033)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@58dc18a6)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@157b674a)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@1c22143e)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@26eb9b60)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@565eef23)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@23510bd6)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@3eb4ec2f)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@3ad16b8d)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@2b6f11ff)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@13c00853)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@6569fcd2)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@7580de42)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@295bf890)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@cfbb1f2)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@3375427b)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@259c3c1c)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@6a95bde4)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@2cc7706c)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@72ea073b)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@181e1b54)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@170ab9fc)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@567f265b)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@50b0891e)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $line52.$read, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $line52.$read, $line52.$read@7700eaa5)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $line52$read, type: class $line52.$read)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@fb723a3)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@596b41b8)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@162cae39)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@943a34)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@3b1102d)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@57186ce2)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@1791f212)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@7d75b692)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@b44d52f)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@1556f758)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@24958f14)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@628ccee8)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@52dafc74)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@7036e0f1)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@320503d3)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@2fd5550f)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@6b8744c1)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@1629d7c5)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@1c56057f)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@141239b4)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@16805673)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@6426afac)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@5ff93095)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@35af55ca)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@2ece2def)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@4a6d1f93)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@40371cc5)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@521ff481)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@428cd0dc)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $line92.$read, name: $iw, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $line92.$read, $line92.$read@103e78fa)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $line92$read, type: class $line92.$read)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@5128c91c)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $iw, name: $outer, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $iw, $iw@ed8d35c)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: $anonfun$1, name: $outer, type: class $iw)\u001b[0;0m",
      "\u001b[1;31m\t- object (class $anonfun$1, <function1>)\u001b[0;0m",
      "\u001b[1;31m\t- element of array (index: 2)\u001b[0;0m",
      "\u001b[1;31m\t- array (class [Ljava.lang.Object;, size 3)\u001b[0;0m",
      "\u001b[1;31m\t- field (class: org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10, name: references$1, type: class [Ljava.lang.Object;)\u001b[0;0m",
      "\u001b[1;31m\t- object (class org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10, <function2>)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)\u001b[0;0m",
      "\u001b[1;31m  ... 85 more\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "//val predictedValue = x: Map[String, Double] => Double = x.get(\"probability_1\")\n",
    "\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "def p(x: Map[String, Double]) = x.get(\"probability_1\")\n",
    "val getProbUdf = udf(p(_: Map[String,Double]))\n",
    "\n",
    "output.withColumn(\"prob\", getProbUdf(output.col(\"prediction\"))).show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3181985082124520989,Map(probability_1 -> 0.10855618862194527, probability_0 -> 0.8914438113780547, rawPrediction_0 -> 1.052787250921204, prediction -> 0.0, rawPrediction_1 -> -1.052787250921204)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.take(1).apply(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine a few features by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### have to create a new workflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val passengersData2 = passengersData.select(\"survived\", \"name\", \"pClass\", \"sex\", \"age\", \"sibSp\", \"parCh\", \"fare\", \"embarked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val columns = passengerData.columns.filter( col => col != \"id\" & col != \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Automated model selection\n",
    "val pred = BinaryClassificationModelSelector().setInput(survived, checkedFeatures).getOutput()\n",
    "//val pred = new OpGBTClassifier().setInput(survived, checkedFeatures).getOutput()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL User Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+-------------------+\n",
      "|        title|sum(1)|                  p|\n",
      "+-------------+------+-------------------+\n",
      "|         Capt|     1|                0.0|\n",
      "|          Col|     2|                0.5|\n",
      "|          Don|     1|                0.0|\n",
      "|           Dr|     7|0.42857142857142855|\n",
      "|     Jonkheer|     1|                0.0|\n",
      "|         Lady|     1|                1.0|\n",
      "|        Major|     2|                0.5|\n",
      "|       Master|    40|              0.575|\n",
      "|         Miss|   182| 0.6978021978021978|\n",
      "|         Mlle|     2|                1.0|\n",
      "|          Mme|     1|                1.0|\n",
      "|           Mr|   517|0.15667311411992263|\n",
      "|          Mrs|   125|              0.792|\n",
      "|           Ms|     1|                1.0|\n",
      "|          Rev|     6|                0.0|\n",
      "|          Sir|     1|                1.0|\n",
      "| the Countess|     1|                1.0|\n",
      "+-------------+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define a regular Scala function\n",
    "val title: String => String = _.split(\"\\\\.\").apply(0).split(\",\").apply(1)\n",
    "\n",
    "// Define a UDF that wraps the upper Scala function defined above\n",
    "// You could also define the function in place, i.e. inside udf\n",
    "// but separating Scala functions from Spark SQL's UDFs allows for easier testing\n",
    "import org.apache.spark.sql.functions.udf\n",
    "val titleUDF = udf(title)\n",
    "\n",
    "// Apply the UDF to change the source dataset\n",
    "passengerData.withColumn(\"title\", titleUDF(passengerData.col(\"name\"))).createOrReplaceTempView(\"title\")\n",
    "\n",
    "spark.sql(\"select title, sum(1), avg(survived) as p from title group by 1 order by 1\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Passenger data schema\n",
    "case class Passenger(\n",
    "  id: Int,\n",
    "  survived: Int,\n",
    "  pClass: Option[Int],\n",
    "  name: Option[String],\n",
    "  sex: Option[String],\n",
    "  age: Option[Double],\n",
    "  sibSp: Option[Int],\n",
    "  parCh: Option[Int],\n",
    "  ticket: Option[String],\n",
    "  fare: Option[Double],\n",
    "  cabin: Option[String],\n",
    "  embarked: Option[String]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import necessary packages\n",
    "import com.salesforce.op.features.FeatureBuilder\n",
    "import com.salesforce.op.features.types._\n",
    "import com.salesforce.op._\n",
    "\n",
    "// Define features using the TransmogrifAI types based on the data\n",
    "val survived = FeatureBuilder.RealNN[Passenger].extract(_.survived.toRealNN).asResponse\n",
    "\n",
    "val pClass = FeatureBuilder.PickList[Passenger].extract(_.pClass.map(_.toString).toPickList).asPredictor\n",
    "\n",
    "val name = FeatureBuilder.Text[Passenger].extract(_.name.toText).asPredictor\n",
    "\n",
    "val sex = FeatureBuilder.PickList[Passenger].extract(_.sex.map(_.toString).toPickList).asPredictor\n",
    "\n",
    "val age = FeatureBuilder.RealNN[Passenger].extract(_.age.toRealNN).asPredictor\n",
    "\n",
    "val sibSp = FeatureBuilder.Integral[Passenger].extract(_.sibSp.toIntegral).asPredictor\n",
    "\n",
    "val parCh = FeatureBuilder.Integral[Passenger].extract(_.parCh.toIntegral).asPredictor\n",
    "\n",
    "val ticket = FeatureBuilder.PickList[Passenger].extract(_.ticket.map(_.toString).toPickList).asPredictor\n",
    "\n",
    "val fare = FeatureBuilder.Real[Passenger].extract(_.fare.toReal).asPredictor\n",
    "\n",
    "val cabin = FeatureBuilder.PickList[Passenger].extract(_.cabin.map(_.toString).toPickList).asPredictor\n",
    "\n",
    "val embarked = FeatureBuilder.PickList[Passenger].extract(_.embarked.map(_.toString).toPickList).asPredictor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
